{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xnli (/Users/leandro.desouza/.cache/huggingface/datasets/xnli/all_languages/1.1.0/243f155ecab4d4f6e82e4eeab62b8c6b1f7abfcb8ed7fcc1661be8e25b117404)\n",
      "100%|██████████| 3/3 [00:00<00:00, 40.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli = load_dataset(\"xnli\", \"all_languages\")\n",
    "xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ar': '- و قد ال كريم المفاهيمية اثنان اساسيين - المنتج والجغرافيا .',\n",
       "  'bg': 'концептуално крем краде има две основни измерения - продукт и география .',\n",
       "  'de': 'Konzeptionell cream abschöpfen hat zwei grundlegende Dimensionen - Produkt und Geographie .',\n",
       "  'el': 'Η εννοιολογικά κρέμα κρέμα έχει δύο βασικές διαστάσεις - προϊόν και γεωγραφία .',\n",
       "  'en': 'Conceptually cream skimming has two basic dimensions - product and geography .',\n",
       "  'es': 'Los robando de crema conceptualmente tienen dos dimensiones básicas : producto y geografía .',\n",
       "  'fr': \"L' écrémage conceptuel de la crème a deux dimensions fondamentales : le produit et la géographie .\",\n",
       "  'hi': 'Conceptually क ् रीम एंजलिस में दो मूल आयाम हैं - उत ् पाद और भूगोल ।',\n",
       "  'ru': 'Концептуально крем крем имеет два основных измерения - продукт и география .',\n",
       "  'sw': 'Sakata cream ya conceptually ina vipimo viwili vya msingi - bidhaa na geography .',\n",
       "  'th': 'ท่า ครีม ยักยอก มี สอง มิติ พื้นฐาน   -   สินค้า และ ภูมิศาสตร์',\n",
       "  'tr': 'Kavramsal krem kaymağını iki temel boyutu vardır - ürün ve coğrafya .',\n",
       "  'ur': 'ارضیات کی ناپیدی اور جغرافیہ نے دو بنیادی فسل تصاویر اور جغرافیہ کا اشتراک کیا ہے ۔',\n",
       "  'vi': 'Conceptually kem skimming có hai kích thước cơ bản - sản phẩm và địa lý .',\n",
       "  'zh': '从 概念 上 看 , 奶油 收入 有 两 个 基本 方面 产品 和 地理 .'},\n",
       " {'ar': 'انت تعرف خلال الموسم و اعتقد انه عند المستوى الخاص بك تفقديهم للمستوى التالي اذا قرروا ان اذكار الفريق الام الذي قرر الشجعان الاتصال به لاستذكر رجل من ثلاث مرات ثم رجل مزدوج يذهب الى استبدلوه وشخص واحد يذهب ليحل محله',\n",
       "  'bg': 'по време на сезона и предполагам , че на твоето ниво ще ги загубиш на следващото ниво , ако те решат да си припомнят отбора на родителите , \" брейвс \" решават да се обади да си припомнят човек от тройно а след това двойно момче отива до смени го и един човек отива да го замести .',\n",
       "  'de': 'Du weißt , während der Saison und ich schätze , auf deiner Ebene verlierst du sie auf die nächste Ebene , wenn sie sich entschließen , sich an das Eltern-Team zu erinnern , entscheiden die braves , einen Kerl von Triple A zu rufen , dann geht ein Double ein Kerl auf die nächste Stufe . Ersetzen sie ihn und ein einziger Mann geht hoch , um ihn zu ersetzen .',\n",
       "  'el': 'Ξέρεις κατά τη διάρκεια της σεζόν και υποθέτω ότι στο επίπεδο σου θα τους χάσεις στο επόμενο επίπεδο αν αποφασίσουν να θυμηθούν τη μητρική ομάδα που οι γενναίοι αποφασίζουν να πάρουν για να θυμηθούν έναν τύπο από το triple a τότε ένας διπλός τύπος πάει πάνω αντικαταστήσει τον και ένας άντρας πάει να τον αντικαταστήσει .',\n",
       "  'en': 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him',\n",
       "  'es': 'Usted sabe durante la temporada y supongo que a su nivel uh los pierde al siguiente nivel si si deciden recordar al equipo de los padres los bravos deciden llamar para recordar a un tipo del triple a y luego un hombre doble se sube a reemplazar y un solo hombre sube para reemplazar .',\n",
       "  'fr': \"Tu sais pendant la saison et je suppose qu' à ton niveau euh tu les perds au niveau suivant si s' ils décident de se rappeler l' équipe des parents les braves décident d' appeler pour rappeler un mec du triple a puis un double un mec monte à remplacez-le et un seul homme monte pour le remplacer .\",\n",
       "  'hi': 'आप मौसम के दौरान जानते हैं और मैं अपने स ् तर पर लगता है कि आप उन ् हें अगले स ् तर पर खो देते हैं यदि वे माता-पिता को याद करने का निर ् णय करते हैं तो braves को एक आदमी को याद करने का निर ् णय लेने का निर ् णय करता है तो एक डबल आदमी के लिए चला जाता है उसे बदलें और एक अकेला आदमी उसे बदलने के लिए चला जाता है',\n",
       "  'ru': 'Вы знаете , что во время сезона , и я думаю , что на вашем уровне вы потеряете их до следующего уровня , если они решат вспомнить команду родителей , которую \" Воинов \" решили позвонить , чтобы вспомнить парня из \" Тройной а а потом двойной парень , который подходит к нему . Замените его , и один парень заменит его .',\n",
       "  'sw': 'Unajua wakati wa msimu na i katika kiwango chako , uh kwa kiwango kinachofuata kama wakiamua kukumbuka timu ya mzazi ambayo braves kuamua kupiga simu ya kukumbuka jamaa wa triple a then a double guy goes up replace him and a single guy anaenda replace',\n",
       "  'th': 'คุณ รู้ ว่า ใน ช่วง ฤดูกาล และ ฉัน เดา ว่า ใน ระดับ ของ คุณ เอ่อ คุณ สูญเสีย พวกเขา ไป ใน ระดับ ถัดไป ถ้า พวกเขา ตัดสินใจ ที่จะ จดจำ ทีม ผู้ปกครอง ผู้ กล้าหาญ ตัดสินใจ ที่จะ โทร ไป จำ ผู้ชาย จาก   triple   a   แล้ว ผู้ชาย สอง คน ขึ้นไป แทนที่ เขา และ ผู้ชาย คน นึง ขึ้นไป แทนที่ เขา',\n",
       "  'tr': 'Sezon boyunca bilirsin ve sanırım senin seviyesinde onları bir sonraki seviyeye kaybedersin . Eğer ana takımı geri almaya karar verirsen yiğitlere , üçlü bir adamı hatırlamak için aramaya karar verir. yerini değiştir ve tek bir adam onun yerine geçecek',\n",
       "  'ur': 'آپ کو معلوم ہے کہ آپ کے طور پر آپ کی سطح پر کم از کم سیکنڈ تک رسائی حاصل کرنے کے لیے آپ کو اس وقت تک رسائی حاصل کرنے کی ضرورت ہے ۔ اس کے بعد ان کی جگہ پر کوئی الزام نہیں ہے',\n",
       "  'vi': 'Bạn biết trong mùa giải và tôi đoán ở mức độ của bạn , bạn sẽ mất chúng đến mức độ tiếp theo nếu họ quyết định nhớ lại đội ngũ cha mẹ các chiến binh quyết định gọi để nhớ lại một người từ ba a sau đó một người đàn ông đi lên đến thay thế anh ta và một người đàn ông nào đó đi lên để thay thế anh ta .',\n",
       "  'zh': '你 知道 在 这个 季节 , 我 猜 在 你 的 水平 你 把 他们 丢到 下 一个 水平 , 如果 他们 决定 召回 的 家长 队 , 勇士 队 决定 打电话 召回 一个 家伙 从 三 个 a , 然后 一个 双人 上 去. 取代 他 和 一个 男人 去 取代 他'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli[\"train\"][:2][\"premise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': ['ar',\n",
       "  'bg',\n",
       "  'de',\n",
       "  'el',\n",
       "  'en',\n",
       "  'es',\n",
       "  'fr',\n",
       "  'hi',\n",
       "  'ru',\n",
       "  'sw',\n",
       "  'th',\n",
       "  'tr',\n",
       "  'ur',\n",
       "  'vi',\n",
       "  'zh'],\n",
       " 'translation': ['المنتج والجغرافيا هو ما يجعل كريم القشط العمل .',\n",
       "  'продукт и география са това , което прави крем краде работа .',\n",
       "  'Produkt und Geographie sind das , was creme abschöpfen Arbeit macht .',\n",
       "  'Το προϊόν και η γεωγραφία είναι αυτά που κάνουν την κρέμα να κλέβει .',\n",
       "  'Product and geography are what make cream skimming work .',\n",
       "  'El producto y la geografía son los que hacen que la crema funcione .',\n",
       "  'Le produit et la géographie sont ce qui fait travailler la crème de la crème .',\n",
       "  'उत ् पाद और भूगोल क ् या क ् रीम एंजलिस काम बनाते हैं .',\n",
       "  'Продукт и география - это то , что делает крем для работы с кремом .',\n",
       "  'Bidhaa na geography ndio hufanya kazi ya utoaji wa cream .',\n",
       "  'สินค้า และ ภูมิศาสตร์ คือ สิ่ง ที่ ทำให้ ครีม ยักยอก งาน',\n",
       "  'Ürün ve coğrafya , krem kaymağını işi yapan şey .',\n",
       "  'مصنوعہ اور جغرافیہ',\n",
       "  'Sản phẩm và địa lý là những gì làm cho kem skimming làm việc .',\n",
       "  '产品 和 地理 是 什么 使 奶油 抹 霜 工作 .']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli[\"train\"][0][\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [(i, lang, trans) for i, ex in enumerate(xnli[\"validation\"]) for lang, trans in zip(ex[\"hypothesis\"][\"language\"], ex[\"hypothesis\"][\"translation\"])]\n",
    "p = [(i, lang, prem) for i, ex in enumerate(xnli[\"validation\"]) for lang, prem in ex[\"premise\"].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37350, 37350)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h), len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 'en', 'He called his mom as soon as the school bus dropped him off.'),\n",
       " (0, 'en', \"And he said, Mama, I'm home.\"),\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[4], p[4], xnli[\"validation\"][h[4][0]][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 2}]\n",
    "\n",
    "{\"a\": i[\"a\"] for i in a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/leandro.desouza/.cache/huggingface/datasets/xnli/all_languages/1.1.0/243f155ecab4d4f6e82e4eeab62b8c6b1f7abfcb8ed7fcc1661be8e25b117404/cache-ae6ba8ae6ab09ee4.arrow\n",
      "Loading cached processed dataset at /Users/leandro.desouza/.cache/huggingface/datasets/xnli/all_languages/1.1.0/243f155ecab4d4f6e82e4eeab62b8c6b1f7abfcb8ed7fcc1661be8e25b117404/cache-850110589f9a32a9.arrow\n",
      "Loading cached processed dataset at /Users/leandro.desouza/.cache/huggingface/datasets/xnli/all_languages/1.1.0/243f155ecab4d4f6e82e4eeab62b8c6b1f7abfcb8ed7fcc1661be8e25b117404/cache-a419da208ab803ac.arrow\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def flat_xnli(examples):\n",
    "    h = [(i, lang, trans) for i, ex in enumerate(examples[\"hypothesis\"]) for lang, trans in zip(ex[\"language\"], ex[\"translation\"])]\n",
    "    p = [(i, lang, prem) for i, ex in enumerate(examples[\"premise\"]) for lang, prem in ex.items()]\n",
    "\n",
    "    features = defaultdict(list)\n",
    "\n",
    "    for (i, l, hy), (_, _, pr) in zip(h, p):\n",
    "\n",
    "        features[\"premise\"].append(pr)\n",
    "        features[\"hypothesis\"].append(hy)\n",
    "        features[\"language\"].append(l)\n",
    "        features[\"label\"].append(examples[\"label\"][i])\n",
    "\n",
    "    return features\n",
    "\n",
    "features = xnli.map(flat_xnli, remove_columns=[\"hypothesis\", \"premise\", \"label\"], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hypothesis': 'Product and geography are what make cream skimming work .',\n",
       "  'label': 1,\n",
       "  'language': 'en',\n",
       "  'premise': 'Conceptually cream skimming has two basic dimensions - product and geography .'},\n",
       " {'hypothesis': 'Produkt und Geographie sind das , was creme abschöpfen Arbeit macht .',\n",
       "  'label': 1,\n",
       "  'language': 'de',\n",
       "  'premise': 'Konzeptionell cream abschöpfen hat zwei grundlegende Dimensionen - Produkt und Geographie .'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[\"train\"][4], features[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hypothesis': 'You lose the things to the following level if the people recall .',\n",
       "  'label': 0,\n",
       "  'language': 'en',\n",
       "  'premise': 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him'},\n",
       " {'hypothesis': 'Man verliert die Dinge auf die folgende Ebene , wenn sich die Leute erinnern .',\n",
       "  'label': 0,\n",
       "  'language': 'de',\n",
       "  'premise': 'Du weißt , während der Saison und ich schätze , auf deiner Ebene verlierst du sie auf die nächste Ebene , wenn sie sich entschließen , sich an das Eltern-Team zu erinnern , entscheiden die braves , einen Kerl von Triple A zu rufen , dann geht ein Double ein Kerl auf die nächste Stufe . Ersetzen sie ihn und ein einziger Mann geht hoch , um ihn zu ersetzen .'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[\"train\"][19], features[\"train\"][17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_sequence(example, premise_column, hypothesis_column):\n",
    "    return (\n",
    "        f\"premise: {example[premise_column]}. hypothesis: {example[hypothesis_column]}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def process_xnli(tokenizer, max_length, target_max_length, dataset):\n",
    "    \"\"\"\n",
    "    Process XNLI for seq2seq models. This functions extracts features in the form:\n",
    "\n",
    "    {\n",
    "        \"input_ids\": [ ... ]\n",
    "        \"attention_mask\": [ ... ] \n",
    "        \"target_ids: [ ... ],\n",
    "        \"label\": [ ... ]\n",
    "    }\n",
    "\n",
    "    The input ids are encoded tokens from the formatted string:\n",
    "\n",
    "        premise: <premise>. hypothesis: <hypothesis>.\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_sample(examples):\n",
    "        input_seq = [f\"premise: {prem}. hypothesis: {hyp}.\" for prem, hyp in zip(examples[\"premise\"], examples[\"hypothesis\"])]\n",
    "        target_seq = [str(l) for l in examples[\"label\"]]\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            input_seq,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "\n",
    "        target_encoded = tokenizer(\n",
    "            target_seq,\n",
    "            max_length=target_max_length,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "\n",
    "        encoded[\"target_ids\"] = target_encoded[\"input_ids\"]\n",
    "        encoded[\"label\"] = examples[\"label\"]\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    examples = dataset.map(\n",
    "        flat_xnli, batched=True, remove_columns=[\"hypothesis\", \"premise\", \"label\"]\n",
    "    )\n",
    "    features = examples.map(\n",
    "        _process_sample,\n",
    "        batched=True,\n",
    "        remove_columns=[\"premise\", \"hypothesis\", \"language\"],\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "max_length = 1024\n",
    "target_max_length = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/leandro.desouza/.cache/huggingface/datasets/xnli/all_languages/1.1.0/243f155ecab4d4f6e82e4eeab62b8c6b1f7abfcb8ed7fcc1661be8e25b117404/cache-a419da208ab803ac.arrow\n",
      "100%|██████████| 38/38 [00:12<00:00,  3.02ba/s]\n"
     ]
    }
   ],
   "source": [
    "xnli_valid_feat = process_xnli(tokenizer, max_length, target_max_length, xnli[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'target_ids'],\n",
       "    num_rows: 37350\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli_valid_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'input_ids': [115,\n",
       "  117,\n",
       "  104,\n",
       "  112,\n",
       "  108,\n",
       "  118,\n",
       "  104,\n",
       "  61,\n",
       "  35,\n",
       "  220,\n",
       "  139,\n",
       "  220,\n",
       "  133,\n",
       "  219,\n",
       "  170,\n",
       "  220,\n",
       "  135,\n",
       "  219,\n",
       "  143,\n",
       "  35,\n",
       "  220,\n",
       "  136,\n",
       "  219,\n",
       "  170,\n",
       "  220,\n",
       "  136,\n",
       "  219,\n",
       "  170,\n",
       "  219,\n",
       "  143,\n",
       "  35,\n",
       "  220,\n",
       "  135,\n",
       "  220,\n",
       "  133,\n",
       "  219,\n",
       "  178,\n",
       "  35,\n",
       "  219,\n",
       "  188,\n",
       "  219,\n",
       "  178,\n",
       "  219,\n",
       "  173,\n",
       "  35,\n",
       "  220,\n",
       "  135,\n",
       "  220,\n",
       "  135,\n",
       "  220,\n",
       "  136,\n",
       "  220,\n",
       "  137,\n",
       "  219,\n",
       "  181,\n",
       "  220,\n",
       "  135,\n",
       "  49,\n",
       "  49,\n",
       "  35,\n",
       "  107,\n",
       "  124,\n",
       "  115,\n",
       "  114,\n",
       "  119,\n",
       "  107,\n",
       "  104,\n",
       "  118,\n",
       "  108,\n",
       "  118,\n",
       "  61,\n",
       "  35,\n",
       "  219,\n",
       "  170,\n",
       "  219,\n",
       "  173,\n",
       "  219,\n",
       "  184,\n",
       "  220,\n",
       "  135,\n",
       "  35,\n",
       "  219,\n",
       "  171,\n",
       "  219,\n",
       "  166,\n",
       "  220,\n",
       "  136,\n",
       "  220,\n",
       "  138,\n",
       "  35,\n",
       "  219,\n",
       "  176,\n",
       "  219,\n",
       "  170,\n",
       "  220,\n",
       "  135,\n",
       "  220,\n",
       "  136,\n",
       "  219,\n",
       "  170,\n",
       "  35,\n",
       "  219,\n",
       "  166,\n",
       "  220,\n",
       "  139,\n",
       "  219,\n",
       "  184,\n",
       "  220,\n",
       "  135,\n",
       "  219,\n",
       "  173,\n",
       "  220,\n",
       "  138,\n",
       "  35,\n",
       "  219,\n",
       "  176,\n",
       "  219,\n",
       "  170,\n",
       "  220,\n",
       "  132,\n",
       "  220,\n",
       "  135,\n",
       "  219,\n",
       "  172,\n",
       "  35,\n",
       "  219,\n",
       "  170,\n",
       "  220,\n",
       "  135,\n",
       "  220,\n",
       "  136,\n",
       "  219,\n",
       "  178,\n",
       "  219,\n",
       "  180,\n",
       "  219,\n",
       "  182,\n",
       "  220,\n",
       "  141,\n",
       "  219,\n",
       "  172,\n",
       "  49,\n",
       "  49,\n",
       "  1],\n",
       " 'label': 1,\n",
       " 'target_ids': [52, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli_valid_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'premise: وقال، ماما، لقد عدت للمنزل.. hypothesis: اتصل بأمه حالما أوصلته حافلة المدرسية..</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(xnli_valid_feat[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([xnli_valid_feat[0][\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [str(l) for l in xnli[\"validation\"][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '0', '1', '2', '0', '2', '0', '1', '0']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dfd2a79c33a006769b4e717450606bc361044920c88babebbbbe6e3f972749c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
