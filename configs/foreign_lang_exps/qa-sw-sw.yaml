seed_everything: 42
data:
  class_path: data_qa.TydiQAGoldPModule
  init_args:
    tokenizer_name: "hugo/byt5-mono-sw-v1"
    batch_size: 6
    max_length: 2048
    max_target_length: 768
    train_language: sw
    dataloader_num_workers: 6
    validate_on:
      - sw
model:
  class_path: model_qa.QuestionAnsweringModel
  init_args:
    pretrained_model_name: "hugo/byt5-mono-sw-v1"
    pretrained_model_revision: "b3c60b74c853d988ac1c425bc174456307f8f1bc"
    use_pretrained_weights: true
    from_flax: true
    max_target_length: 768
    metric_name: squad
optimizer:
  class_path: transformers.optimization.Adafactor
  init_args:
    lr: 0.0001
    scale_parameter: false
    relative_step: false
trainer:
  gpus: 1
  max_epochs: 10
  accumulate_grad_batches: 4
  precision: 32
  val_check_interval: 1.0
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: qa
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        filename: "baseline-qa-sw-sw-epoch={epoch}-step={step}-acc={val/sw/f1:.4f}"
        monitor: "val/sw/f1"
        mode: "max"
        save_last: true
        auto_insert_metric_name: false
    - class_path: pytorch_lightning.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: "val/sw/f1"
        mode: "max"
        patience: 3
